---
title: "P8105 Homework 6"
output: github_document
date: "2023-11-24"
---

## Library and Document-Wide Settings Setup
Loading libraries and establishing some document-wide settings for later use.

!!! CHECK THAT DON'T HAVE UNNECESSARY LIBRARIES !!!!!

```{r libraries}
library(tidyverse)
library(modelr)
library(mgcv)
library(viridis)
```

```{r setup, include = FALSE}
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 2

## Importing Central Park Weather Data

First, I'm importing the weather data for Central Park.

```{r}
weather = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## Generating Linear Models Then Calculating R-Square and Log(Beta_1*Beta_2)

Next, I'm generating 5000 bootstrap samples and fitting a linear regression model to each, using `tmax` as the response with `tmin` and `prcp` as the predictors. Then, with the linear model generated for each sample, I'm pulling out only the `r_squared` and `log(beta_1*beta_2)` values.

!!!! IS THERE A BETTER WAY TO CREATE THE LOG PRODUCT VARIABLE AND ALSO KEEP THE R-SQUARED VARIABLE?

```{r}
boot_straps = 
  weather |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    term_results = map(models, broom::tidy),
    glance_results = map(models, broom::glance)
  ) |> 
  unnest(term_results) |> 
  select(.id, term, estimate, glance_results) |> 
  unnest(glance_results) |> 
  janitor::clean_names() |> 
  select(id:r_squared) |> 
  group_by(id) |> 
  summarize(
    log_product = log(estimate[2]) + log(estimate[3]),
    r_squared = r_squared
  ) |> 
  distinct()


```

## Plotting the Distribution of the Estimates

```{r}
boot_straps |> 
  ggplot(aes(x = r_squared)) +
  geom_density() + 
  labs(
    title = "Distribution of R-Squared Values",
    x = "R-Squared",
    y = "Density"
  )

boot_straps |> 
  ggplot(aes(x = log_product)) +
  geom_density() + 
  labs(
    title = "Distribution of Log(β1*β2)",
    x = "Log(β1*β2)",
    y = "Density"
  )

```

The distribution of the r-squared value is slightly left skewed, with the most common r-squared value landing at just above 0.92. The vast majority of r-squared values appear to fall between approximately 0.88 and 0.95. As a value of 1 would indicate that 100% of the variance in maximum temperature is explained by the model, this distribution of r-squared values suggests that the model using  minimum temperature and precipitation does a fairly solid job of predicting the maximum temperature. 

!!!!!! DESCRIBE THE LOG(B1*B2) GRAPH IN WORDS
ALSO TALK ABOUT THE PROPORTION OF SAMPLES THAT HAD AN NAN VALUE, AND THAT THE DISTRIBUTION DOESN'T INCLUDE THOSE NAs
ALSO UPDATE R-HAT AND BETA VALUES TO USE THE ACTUAL SYMBOLS (GOOGLE HOW TO DO THIS IN R MARKDOWN)

## Generating 95% Confidence Intervals

I can use the estimates across my 5000 bootstrap samples to construct a 95% confidence interval for r-squared and log(beta_1*beta_2). To do so, I need to identify the upper and lower 2.5% of the distribution for each across all of the samples. 

95% CI for r-squared: (`r boot_straps |> pull(r_squared) |> quantile(0.025, na.rm = TRUE) |> round(digits = 4)`, `r boot_straps |> pull(r_squared) |> quantile(0.975, na.rm = TRUE) |> round(digits = 4)`).  
95% CI for log(beta_1*beta_2): (`r boot_straps |> pull(log_product) |> quantile(0.025, na.rm = TRUE) |> round(digits = 4)`, `r boot_straps |> pull(log_product) |> quantile(0.975, na.rm = TRUE) |> round(digits = 4)`). 
 

# Problem 3

## Importing and Cleaning the Birthweight Data Set

Loading and cleaning the `birthweight` dataset in preparation for regression analysis. Specifically:

* Converting the `babysex`, `malform`, `frace`, and `mrace` variables to factors
* Although the `pnumlbw` and `pnumgsa` variables both have a value of 0 for all observations, not dropping them as I'm not certain that this actually indicates missingness as opposed to an accurate value of 0

```{r}

birthweight = 
  read_csv("Data/birthweight.csv") |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    babysex = as.factor(case_match(babysex,
                                   1 ~ "Male",
                                   2 ~ "Female")),
    malform = as.factor(case_match(malform,
                                   0 ~ "Absent",
                                   1 ~"Present")),
    frace = as.factor(case_match(frace,
                                 1 ~ "White",
                                 2 ~ "Black",
                                 3 ~ "Asian",
                                 4 ~ "Puerto Rican",
                                 8 ~ "Other",
                                 9 ~ "")),
    mrace = as.factor(case_match(mrace,
                                 1 ~ "White",
                                 2 ~ "Black",
                                 3 ~ "Asian",
                                 4 ~ "Puerto Rican",
                                 8 ~ "Other",
                                 9 ~ "")),
    frace = fct_infreq(frace),
    mrace = fct_infreq(mrace)
  ) |> 
  select(bwt, everything())

```

```{r}
birthweight |> 
  group_by(mrace) |> 
  summarize(
    count = n()
  )
```


## Fitting a Birthweight Linear Regression Model

### Model 1: All Predictors

Based on information from the [Cleveland Clinic](https://my.clevelandclinic.org/health/diseases/24980-low-birth-weight), the following risk factors are likely predictors of birthweight: 

* Preterm birth (variable: `gaweeks`)
* Being younger than 18 or older than 34 (variable: `momage`)
* Smoking during pregnancy (variable: `smoken`)
* Not gaining enough weight during pregnancy (variable: `wtgain`)
* Being Black (variable: `mrace`)

As a result, they will likely be in my final linear regression model. But first, I'll create an initial model with all of my non-birthweight variables as predictors. I'll then create a plot of the residuals vs. predicted values.

```{r}
fit_1 = 
  lm(
    bwt ~ babysex + bhead + blength + fincome + frace + gaweeks + malform 
    + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi 
    + ppwt + smoken + wtgain, 
    data = birthweight
  )

fit_1 |> 
  broom::tidy() |> 
  mutate(
    term = str_replace(term, "^mrace", "Mother's Race: "),
    term = str_replace(term, "^frace", "Father's Race: "),
    term = str_replace(term, "malform", "Malformation: "),
    term = str_replace(term, "babysex", "Baby Sex: ")
  ) |> 
  knitr::kable(digits = 3)


birthweight |> 
  modelr::add_residuals(fit_1) |> 
  mutate(
    predicted = predict(fit_1)
  ) |> 
  filter(resid < 1000) |> 
  ggplot(aes(x = predicted, y = resid)) + 
  geom_point()

```
  
From the residuals plot, we can see that while the residuals are generally randomly scattered around zero for predicted birthweight values above approximately 2000 grams, the residuals for birthweights lower than 2000 grams display a problematically-linear distribution. 

From the table displaying the beta estimates for the predictors, we can see that the the indicator with the highest p-value is Father's Race: Other, indicating that it's an insignificant predictor of birthweight. In fact, the p-values for all father's race categories, compared to the reference race, are quite large and insignificant. Additionally, because the `pnumlbw` and `pnumsga` variables have values of 0 for all observations, they have incalculable beta estimates and p-values. So, for my next model, I will drop these 3 variables. 

### Model 2: Dropping `frace`, `pnumlbw`, `pnumsga`, and Dichotomizing Mother's Race

I noticed in the output table from the original model that there's a clear division in the significance of mother's race, with Black having a significant p-value of 0.001, while Puerto Rican has a p-value of 0.211 and Asian has a p-value of 0.204. Because of this, I will recode mother's race into a dichotomous variable of Black/Other Races and use this dichotomized version going forward.

```{r}
birthweight_updated = 
  birthweight |> 
  mutate(
    mrace = as.factor(if_else(as.character(mrace) == "Black", "Black", "Other")),
    mrace = fct_infreq(mrace)
  )
```

Now, as stated above, I will drop `frace`, `pnumlbw`, and `pnumsga` from this model.

```{r}
fit_2 = 
  lm(
    bwt ~ babysex + bhead + blength + fincome + gaweeks + malform + menarche 
    + mheight + momage + mrace + parity + ppbmi + ppwt + smoken + wtgain, 
    data = birthweight_updated
  )

fit_2 |> 
  broom::tidy() |> 
  mutate(
    term = str_replace(term, "^mrace", "Mother's Race: "),
    term = str_replace(term, "malform", "Malformation: "),
    term = str_replace(term, "babysex", "Baby Sex: ")
  ) |> 
  knitr::kable(digits = 3)

birthweight_updated |> 
  modelr::add_residuals(fit_2) |> 
  mutate(
    predicted = predict(fit_2)
  ) |> 
  ggplot(aes(x = predicted, y = resid)) + 
  geom_point()
```

From the residuals plot, we can see that it looks quite similar to the prior model, with residuals being randomly scattered around 0 except for predicted birthweights of less than 2000 grams. From looking at the p-values of the predictor terms, it appears that `ppwt`, `malform`, and `ppbmi` are now the least significant predictors of birthweight, so I will remove those variables from the next model.

### Model 3: Dropping `ppwt`, `malform`, `ppbmi`

Now dropping `ppwt`, `malform`, and `ppbmi` from the model.

```{r}
fit_3 = 
  lm(
    bwt ~ babysex + bhead + blength + fincome + gaweeks + menarche 
    + mheight + momage + mrace + parity + smoken + wtgain, 
    data = birthweight_updated
  )

fit_3 |> 
  broom::tidy() |> 
  mutate(
    term = str_replace(term, "^mrace", "Mother's Race: "),
    term = str_replace(term, "babysex", "Baby Sex: ")
  ) |> 
  knitr::kable(digits = 3)

birthweight_updated |> 
  modelr::add_residuals(fit_3) |> 
  mutate(
    predicted = predict(fit_3)
  ) |> 
  ggplot(aes(x = predicted, y = resid)) + 
  geom_point()
```











Start with the full model (with every predictor), then incrementally drop the least significant predictor. After each drop, rerun the new slimmed down model and look at the plot of the residuals.

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.


`gaweeks`
`momage`
`smoken`
`wtgain`
`mrace`


